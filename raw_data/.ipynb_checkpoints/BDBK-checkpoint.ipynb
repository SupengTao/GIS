{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_path = 'D:\\\\CKG\\\\raw_data\\\\all_city.txt'\n",
    "river_path = 'D:\\\\CKG\\\\raw_data\\\\river.txt'\n",
    "school_path = 'D:\\\\CKG\\\\raw_data\\\\all_school.txt'\n",
    "spot_path = 'D:\\\\CKG\\\\raw_data\\\\all_spot.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citys():\n",
    "    citys = []\n",
    "    with open (city_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            citys.extend(line.split())\n",
    "    return citys\n",
    "def get_rivers():\n",
    "    rivers = []\n",
    "    with open(river_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            rivers.extend(line.split('、'))\n",
    "    return rivers\n",
    "\n",
    "def get_schools():\n",
    "    schools = []\n",
    "    with open (school_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            schools.extend(line.split())\n",
    "    return schools\n",
    "def get_spots():\n",
    "    spots = []\n",
    "    with open (spot_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            spots.extend(line.split())\n",
    "    return spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "citys = get_citys()\n",
    "rivers = get_rivers()\n",
    "schools = get_schools()\n",
    "spots = get_spots()\n",
    "start_url = 'https://baike.baidu.com/item/'\n",
    "city_urls = [start_url + city for city in citys]\n",
    "river_urls = [start_url + river for river in rivers]\n",
    "school_urls = [start_url + school for school in schools]\n",
    "spot_urls = [start_url + spot for spot in spots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.10 Safari/537.36'}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, headers):\n",
    "    try:\n",
    "        r = requests.get(url,headers = headers)\n",
    "        r.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    else:\n",
    "        html = r.text.encode(r.encoding).decode()\n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        name = soup.find_all('dt',class_= 'basicInfo-item name')\n",
    "        values = soup.find_all('dd',class_= 'basicInfo-item value')\n",
    "        names = list(map(lambda key:key.text.replace('\\xa0',''), name))\n",
    "        values = list(map(lambda value: re.sub('\\\\n\\[.*]', '',value.text.strip()), values))\n",
    "        data = list(zip(names, values))\n",
    "        data = {name:value for name, value in data}\n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_city_data = []\n",
    "# for idx, url in enumerate(city_urls):\n",
    "#     data = get_data(url, headers)\n",
    "#     all_city_data.append(data)\n",
    "#     print(\"finish {}\\{}\".format(idx, len(city_urls)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_river_data = []\n",
    "for idx, url in enumerate(river_urls):\n",
    "    data = get_data(url, headers)\n",
    "    all_river_data.append(data)\n",
    "    print(\"finish {}\\{}\".format(idx, len(river_urls)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_school_data = []\n",
    "for idx, url in enumerate(school_urls):\n",
    "    data = get_data(url, headers)\n",
    "    all_school_data.append(data)\n",
    "    print(\"finish {}\\{}\".format(idx, len(school_urls)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_spot_data = []\n",
    "# for idx, url in enumerate(spot_urls):\n",
    "#     data = get_data(url, headers)\n",
    "#     if data is None:\n",
    "#         continue\n",
    "#     all_spot_data.append(data)\n",
    "#     print(\"finish {}\\{}\".format(idx, len(spot_urls)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def data2json(data,prefix):\n",
    "    data_json = json.dumps(data,ensure_ascii=False)\n",
    "    json.dump(data_json,open('./{}_data.json'.format(prefix), 'w'))\n",
    "def json2data(prefix):\n",
    "    data_json = json.load(open('./{}_data.json'.format(prefix)))\n",
    "    return json.loads(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './city_data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a6fe2dfcec97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# for relaiton extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcity_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson2data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'city'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mspot2city\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcity_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4ae9d8c823a8>\u001b[0m in \u001b[0;36mjson2data\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./{}_data.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjson2data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./{}_data.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './city_data.json'"
     ]
    }
   ],
   "source": [
    "# for relaiton extraction\n",
    "\n",
    "city_data = json2data('city')\n",
    "spot2city = {}\n",
    "for city in city_data:\n",
    "    try:\n",
    "        spots = city.get('著名景点').split('、')[:-1]\n",
    "        for spot in spots:\n",
    "            spot2city[spot] = city.get('中文名称')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实体1， 实体2， 关系， 种类1， 种类2\n",
    "relation1 = '位于'\n",
    "relation2 = '著名景点'\n",
    "type1 = '景点'\n",
    "type2 = '城市'\n",
    "relation_path1 = 'D:\\\\CKG\\\\raw_data\\\\spot_city_relation.txt'\n",
    "relation_path2 = 'D:\\\\CKG\\\\raw_data\\\\city_spot_relation.txt'\n",
    "f1 = open(relation_path1, 'w')\n",
    "f2 = open(relation_path2, 'w')\n",
    "for spot in all_spot:\n",
    "    try:\n",
    "        city = spot2city[spot]\n",
    "        if '中山' in city or '威海' in city or '，' in spot or ',' in spot:\n",
    "            print(spot)\n",
    "            continue\n",
    "        f1.write(\"{},{},{},{},{}\\n\".format(spot,city,relation1,type1,type2))\n",
    "        f2.write(\"{},{},{},{},{}\\n\".format(city,spot,relation2,type2,type1))\n",
    "    except:\n",
    "        print(city)\n",
    "f1.close()\n",
    "f2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
